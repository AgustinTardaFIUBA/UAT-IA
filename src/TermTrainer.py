import gc
import os
import logging
import shutil
import json
import numpy as np
import tensorflow as tf
import keras_tuner as kt
from memory_profiler import profile
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertModel  # Importamos transformers
from tensorflow.keras import backend as backend

from Model import MyHyperModel
from Database.Keyword import Keyword

tf.get_logger().setLevel(logging.ERROR)

class TermTrainer:
    def __init__(self, thesaurus, database):
        self.thesaurus = thesaurus
        # { 'term_id': { 'child_term_id': keyword_index } }. This is for retrieving the index of the term_id children id in the training input for the term_id
        self.keywords_by_term = {}
        # Quantity of models created
        self.models_created = 0
        #Flag for making hyperparameter tuning
        self.hyperparameter_tuning = True
        # db connection
        self.database = database

        # Logging, change log level if needed
        logging.basicConfig(filename='trainer.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
        self.log = logging.getLogger('my_logger')

    # Getters
    def get_keywords_by_term(self):
        return self.keywords_by_term
    
    def get_models_created(self):
        return self.models_created
    
    '''
        Creates the input data for the training as two arrays:
        - texts: array of texts for traning
        - keywords_by_text: array of arrays of keywords for each text. 1 if it matches the keyword, 0 if not
        - keywords_indexes: The index of the keyword matches the position of the training input { 'term_id': index }
    '''
    def create_data_input(self, term_id, children, training_input_creator):
        # keywords_indexes: The index of the keyword matches the position of the training input { 'term_id': index }
        # E.g. with term_id 104: {'102': 0, '1129': 1, '1393': 2, '661': 3}
        keywords_indexes = {}
        keywords = []

        for i in range(len(children)):
            child = children[i]
            # Check if the term_files is None. If it is, it means that the term doesn't have files
            if children[i] is not None:
                keywords_indexes[child] = i

                keyword = self.thesaurus.get_by_id(child).get_name()
                keywords.append(keyword)

        self.log.info(f"Keywords indexes: {json.dumps(keywords_indexes)}")
        self.keywords_by_term[term_id] = keywords_indexes

        # If it doesnt have keywords, the term id is not trainable
        if len(keywords) == 0:
            return [], [], {}

        keyword_db = Keyword(self.database)
        # files_input: { 'file_path': [0, 0, 1, 0] }. The array of 0s and 1s represents the keywords for the file
        files_input = {}
        for child in children:
            # Get all children recursively from the child term (To associate all child files to the term child)
            term_children = self.thesaurus.get_branch_children(child)
            term_children_ids = [term.get_id() for term in term_children]
            term_children_ids.insert(0, child)

            files_paths = keyword_db.get_file_ids_by_keyword_ids(term_children_ids)
            for file_path in files_paths:
                # If the file_path is not in files_input dictionary, creates a new item with the path as the key and an input array filled with 0s
                if file_path not in files_input:
                    files_input[file_path] = [0] * len(children)
                files_input[file_path][keywords_indexes[child]] = 1

        # keywords_by_texts is an array where each document represents, on the set of children that is being trained, 
        # a 1 if it belongs to the category of the child of that position, or a 0 if it doesn't belong
        print("LEN CHILDREN",len(children))
        texts, keywords_by_text = training_input_creator.create_input_arrays(files_input, keywords)

        return texts, keywords_by_text

    # Print memory usage in function
    # @profile
    def generate_model_for_group_of_terms(self, texts, keywords_by_text, term_id, training_input_creator):
        self.log.info(f"Training with {len(texts)} files")

        # Resets all state generated by Keras for memory consumption
        tf.keras.backend.clear_session()
        tf.get_logger().setLevel('ERROR')

        number_of_categories = len(keywords_by_text[0])

        # Tokenización utilizando el tokenizer de BERT
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Usamos el tokenizer de BERT
        sequences = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')

        input_ids = sequences['input_ids'].numpy()
        attention_masks = sequences['attention_mask'].numpy()
        
        # Verificamos si hay suficientes datos para hacer el split
        if len(texts) <= 2 or len(keywords_by_text) <= 2:            
            print("Warning: Not enough data to perform a meaningful train-test split.")
            self.log.warning(f"Not enough data to perform a meaningful train-test split for term ID: {term_id}")
        else:
            print("----------------------------------------------------------------")
            print("Shape of input_ids:", input_ids.shape)
            print("Shape of attention_mask:", attention_masks.shape)
            print("----------------------------------------------------------------")


            train_data, test_data, train_labels, test_labels = train_test_split(input_ids, keywords_by_text, test_size=0.2, random_state=42)
            train_attention_mask, test_attention_mask = train_test_split(attention_masks, test_size=0.2, random_state=42)

            # Convert the data to numpy arrays
            train_labels = np.array(train_labels)
            test_labels = np.array(test_labels)

            # Crear y entrenar el modelo basado en transformers
            model = self.create_transformer_model(number_of_categories, train_data, train_labels, test_data, test_labels,train_attention_mask, test_attention_mask)

            # Guardamos el modelo entrenado
            self.save_trained_model(term_id, model, training_input_creator.get_folder_name())

            # Clear TensorFlow session again to free memory
            tf.keras.backend.clear_session()

            # Remove the models from memory
            del model
            del train_data
            del test_data
            del train_labels
            del test_labels
            del input_ids

            gc.collect()

    def create_transformer_model(self, number_of_categories, train_data, train_labels, test_data, test_labels, train_attention_mask, test_attention_mask):
        # Cargar el modelo preentrenado de BERT
        transformer_model = TFBertModel.from_pretrained('bert-base-uncased')

        # Construir el modelo de clasificación sobre BERT
        input_ids = tf.keras.layers.Input(shape=(train_data.shape[1],), dtype=tf.int32, name="input_ids")
        attention_mask = tf.keras.layers.Input(shape=(train_data.shape[1],), dtype=tf.int32, name="attention_mask")

        bert_output = transformer_model(input_ids, attention_mask=attention_mask)[0]
        cls_token = bert_output[:, 0, :]  # CLS token para la clasificación

        dense = tf.keras.layers.Dense(128, activation='relu')(cls_token)
        output = tf.keras.layers.Dense(number_of_categories, activation='sigmoid')(dense)

        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

        # Compilamos el modelo
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
                      loss='binary_crossentropy',  # Cambiamos la pérdida para multi-label
                      metrics=['accuracy'])

        # Entrenamos el modelo
        print("train_data:", train_data)
        print("attention_mask:", attention_mask)
        print("train_labels:", train_labels)
        print("test_data:", test_data)
        print("test_labels:", test_labels)

        model.fit([train_data, train_attention_mask], train_labels, epochs=5, batch_size=8, validation_data=([test_data, test_attention_mask], test_labels), verbose=1)

        return model

    # Método para guardar el modelo entrenado
   
    def train_group(self, term_id, children, training_input_creator):
        texts, keywords_by_text = self.create_data_input(term_id, children, training_input_creator)

        if len(keywords_by_text):
            self.generate_model_for_group_of_terms(texts, keywords_by_text, term_id, training_input_creator)
            self.models_created += 1

    # Entrypoint method
    def train_model(self, term_id, training_input_creator):
        self.log.info(f"---------------------------------")
        self.log.info(f"Started training for term ID: {term_id}")
        # Check if the term is already trained
        term_is_trained = False
        folder_name = training_input_creator.get_folder_name()
        if os.path.exists('./models/' + folder_name):
            if os.path.exists(f"./models/{folder_name}/{term_id}.keras"):
                self.log.info(f"Model for term {term_id} already exists")
                term_is_trained = True

        children = self.thesaurus.get_by_id(term_id).get_children()
        if not children:
            self.log.info(f"Term {term_id} has no children")
            return
        
        if (not term_is_trained):
            self.train_group(term_id, children, training_input_creator)
            

    def save_trained_model(self, term_id, model, folder_name):
        # Create folder if it doesn't exist
        if not os.path.exists('./models/' + folder_name):
            os.makedirs('./models/' +  folder_name)

        if model is not None:
            model_save_path = f"./models/{folder_name}/{term_id}.keras"
            model.save(model_save_path)

        self.log.info(f"Model saved at: {model_save_path}")
